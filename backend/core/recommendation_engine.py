import os
import json
import sqlite3
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Tuple
from sentence_transformers import SentenceTransformer
import faiss
from datetime import datetime, timedelta
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
import logging

logger = logging.getLogger(__name__)

class ModernRecommendationEngine:
    def __init__(self, db_path: str = None, model_cache_dir: str = None):
        self.db_path = db_path or "papers.db"
        self.model_cache_dir = model_cache_dir or "models"
        
        os.makedirs(self.model_cache_dir, exist_ok=True)
        
        logger.info("SPECTER2 ëª¨ë¸ ë¡œë”© ì¤‘...")
        try:
            # SPECTER2: ë…¼ë¬¸ íŠ¹í™” ì„ë² ë”© ëª¨ë¸
            self.specter_model = SentenceTransformer('allenai/specter2')
            logger.info("âœ… SPECTER2 ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            logger.error(f"SPECTER2 ë¡œë“œ ì‹¤íŒ¨: {e}")
            # SciBERTë¡œ í´ë°±
            try:
                self.specter_model = SentenceTransformer('allenai/scibert_scivocab_uncased')
                logger.info("âœ… SciBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ (í´ë°±)")
            except Exception as e2:
                logger.error(f"SciBERTë„ ì‹¤íŒ¨: {e2}")
                # all-MiniLMìœ¼ë¡œ ìµœì¢… í´ë°±
                self.specter_model = SentenceTransformer('all-MiniLM-L6-v2')
                logger.info("âœ… MiniLM ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ìµœì¢… í´ë°±)")
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.paper_embeddings = None
        self.paper_ids = []
        self.papers_df = None
        self.faiss_index = None
        self.paper_clusters = None
        self.cluster_model = None
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ë°ì´í„°
        self.user_preferences = {}
        self.paper_popularity = {}
        
        logger.info("ğŸ¯ í˜„ëŒ€ì  ì¶”ì²œ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_papers_from_db(self, limit: int = None) -> pd.DataFrame:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            query = """
            SELECT paper_id, title, abstract, authors, categories, 
                   published_date, updated_date
            FROM papers 
            WHERE abstract IS NOT NULL AND title IS NOT NULL
            ORDER BY published_date DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            logger.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {len(df)}ê°œ ë…¼ë¬¸ ë¡œë“œ")
            return df
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def generate_paper_embeddings(self, papers_df: pd.DataFrame) -> np.ndarray:
        """SPECTER2ë¡œ ê³ í’ˆì§ˆ ë…¼ë¬¸ ì„ë² ë”© ìƒì„±"""
        logger.info("ğŸ§  SPECTER2 ì„ë² ë”© ìƒì„± ì‹œì‘...")
        
        # ë…¼ë¬¸ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ì´ˆë¡)
        paper_texts = []
        for _, row in papers_df.iterrows():
            # SPECTER2ì— ìµœì í™”ëœ í˜•ì‹
            text = f"Title: {row['title']}\nAbstract: {row['abstract']}"
            paper_texts.append(text)
        
        # SPECTER2 ì„ë² ë”© ìƒì„±
        embeddings = self.specter_model.encode(
            paper_texts,
            batch_size=8,  # ë©”ëª¨ë¦¬ ìµœì í™”
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”
        )
        
        self.paper_embeddings = embeddings
        self.paper_ids = papers_df['paper_id'].tolist()
        
        logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}")
        return embeddings

    def build_faiss_index(self, embeddings: np.ndarray):
        """Faissë¡œ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        logger.info("âš¡ Faiss ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # Inner Product ì¸ë±ìŠ¤ (ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼)
        index = faiss.IndexFlatIP(embeddings.shape[1])
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU ì¸ë±ìŠ¤ ì‚¬ìš©
        try:
            if faiss.get_num_gpus() > 0:
                index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index)
                logger.info("ğŸš€ GPU ê°€ì† ì¸ë±ìŠ¤ ì‚¬ìš©")
        except:
            logger.info("ğŸ’» CPU ì¸ë±ìŠ¤ ì‚¬ìš©")
        
        index.add(embeddings.astype('float32'))
        
        self.faiss_index = index
        logger.info(f"âœ… Faiss ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")

    def create_paper_clusters(self, embeddings: np.ndarray, n_clusters: int = 50):
        """ë…¼ë¬¸ì„ ì£¼ì œë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§"""
        effective_n_clusters = max(1, min(n_clusters, len(embeddings) // 2))
        logger.info(f"ğŸ¯ ë…¼ë¬¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (k={effective_n_clusters})...")
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        self.cluster_model = KMeans(n_clusters=effective_n_clusters, random_state=42, n_init=10)
        cluster_labels = self.cluster_model.fit_predict(embeddings)
        
        self.paper_clusters = cluster_labels
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ì¸ê¸°ë„ ê³„ì‚°
        self._calculate_cluster_popularity()
        
        logger.info(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {effective_n_clusters}ê°œ ì£¼ì œ í´ëŸ¬ìŠ¤í„°")

    def _calculate_cluster_popularity(self):
        """í´ëŸ¬ìŠ¤í„°ë³„ ì¸ê¸°ë„ ê³„ì‚° (ìµœì‹ ì„± + ë‹¤ì–‘ì„± ê¸°ë°˜)"""
        cluster_popularity = {}
        
        for cluster_id in np.unique(self.paper_clusters):
            cluster_papers = [i for i, c in enumerate(self.paper_clusters) if c == cluster_id]
            
            # ìµœì‹ ì„± ì ìˆ˜ ê³„ì‚°
            recency_scores = []
            for paper_idx in cluster_papers:
                try:
                    paper_date = datetime.fromisoformat(
                        self.papers_df.iloc[paper_idx]['published_date'].replace('Z', '+00:00')
                    )
                    days_old = (datetime.now() - paper_date.replace(tzinfo=None)).days
                    recency_score = max(0.1, 1.0 - (days_old / 365))  # 1ë…„ìœ¼ë¡œ ì •ê·œí™”
                    recency_scores.append(recency_score)
                except:
                    recency_scores.append(0.5)
            
            # í´ëŸ¬ìŠ¤í„° ì¸ê¸°ë„ = í‰ê·  ìµœì‹ ì„± + ë…¼ë¬¸ ìˆ˜ ê°€ì¤‘ì¹˜
            avg_recency = np.mean(recency_scores)
            size_weight = min(1.0, len(cluster_papers) / 10)  # 10ê°œ ì´ìƒì´ë©´ ìµœëŒ€ ê°€ì¤‘ì¹˜
            
            cluster_popularity[cluster_id] = avg_recency * 0.7 + size_weight * 0.3
        
        self.cluster_popularity = cluster_popularity

    def get_content_based_recommendations(self, paper_id: str, n_recommendations: int = 10) -> List[Dict]:
        """SPECTER2 ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¶”ì²œ"""
        try:
            if paper_id not in self.paper_ids:
                logger.warning(f"âŒ ë…¼ë¬¸ ID {paper_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            paper_idx = self.paper_ids.index(paper_id)
            query_embedding = self.paper_embeddings[paper_idx:paper_idx+1]
            
            # Faissë¡œ ìœ ì‚¬ ë…¼ë¬¸ ê²€ìƒ‰
            scores, indices = self.faiss_index.search(
                query_embedding.astype('float32'),
                n_recommendations + 1  # ìê¸° ìì‹  ì œì™¸
            )
            
            recommendations = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx != paper_idx:  # ìê¸° ìì‹  ì œì™¸
                    recommendations.append({
                        'paper_id': self.paper_ids[idx],
                        'similarity_score': float(score),
                        'rank': len(recommendations) + 1,
                        'method': 'semantic_similarity'
                    })
                
                if len(recommendations) >= n_recommendations:
                    break
            
            logger.info(f"ğŸ¯ ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¶”ì²œ {len(recommendations)}ê°œ ìƒì„±")
            return recommendations
            
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []

    def get_cluster_based_recommendations(self, paper_id: str, n_recommendations: int = 10) -> List[Dict]:
        """í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì¶”ì²œ (ì£¼ì œë³„ í˜‘ì—… í•„í„°ë§)"""
        try:
            if paper_id not in self.paper_ids or self.paper_clusters is None:
                return []
            
            paper_idx = self.paper_ids.index(paper_id)
            paper_cluster = self.paper_clusters[paper_idx]
            
            # ê°™ì€ í´ëŸ¬ìŠ¤í„°ì˜ ë…¼ë¬¸ë“¤ ì°¾ê¸°
            cluster_papers = []
            for i, cluster_id in enumerate(self.paper_clusters):
                if cluster_id == paper_cluster and i != paper_idx:
                    # ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš©
                    try:
                        paper_date = datetime.fromisoformat(
                            self.papers_df.iloc[i]['published_date'].replace('Z', '+00:00')
                        )
                        days_old = (datetime.now() - paper_date.replace(tzinfo=None)).days
                        time_weight = max(0.1, 1.0 - (days_old / 730))
                        
                        # í´ëŸ¬ìŠ¤í„° ì¸ê¸°ë„ì™€ ì‹œê°„ ê°€ì¤‘ì¹˜ ê²°í•©
                        cluster_pop = self.cluster_popularity.get(paper_cluster, 0.5)
                        final_score = time_weight * 0.6 + cluster_pop * 0.4
                        
                        cluster_papers.append({
                            'paper_id': self.paper_ids[i],
                            'cluster_score': final_score,
                            'cluster_id': int(paper_cluster),
                            'time_weight': time_weight
                        })
                    except:
                        continue
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            cluster_papers.sort(key=lambda x: x['cluster_score'], reverse=True)
            
            recommendations = []
            for i, rec in enumerate(cluster_papers[:n_recommendations]):
                rec['rank'] = i + 1
                rec['method'] = 'cluster_based'
                recommendations.append(rec)
            
            logger.info(f"ğŸ¯ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì¶”ì²œ {len(recommendations)}ê°œ ìƒì„±")
            return recommendations
            
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []

    def get_hybrid_recommendations(self, paper_id: str, n_recommendations: int = 10) -> List[Dict]:
        """ê³ ê¸‰ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (ì˜ë¯¸ì  + í´ëŸ¬ìŠ¤í„° + ìµœì‹ ì„±)"""
        try:
            # ê° ë°©ë²•ë³„ ì¶”ì²œ ìƒì„±
            content_recs = self.get_content_based_recommendations(paper_id, n_recommendations * 2)
            cluster_recs = self.get_cluster_based_recommendations(paper_id, n_recommendations * 2)
            
            # ì ìˆ˜ í†µí•© (ë” ì •êµí•œ ê°€ì¤‘ì¹˜)
            paper_scores = {}
            
            # ì˜ë¯¸ì  ìœ ì‚¬ì„± ì ìˆ˜ (ê°€ì¤‘ì¹˜ 0.6)
            for rec in content_recs:
                paper_id_rec = rec['paper_id']
                score = rec['similarity_score'] * 0.6
                paper_scores[paper_id_rec] = {
                    'total_score': score,
                    'semantic_score': rec['similarity_score'],
                    'cluster_score': 0,
                    'methods': ['semantic']
                }
            
            # í´ëŸ¬ìŠ¤í„° ì ìˆ˜ (ê°€ì¤‘ì¹˜ 0.4)
            max_cluster_score = max([r['cluster_score'] for r in cluster_recs]) if cluster_recs else 1
            for rec in cluster_recs:
                paper_id_rec = rec['paper_id']
                normalized_score = rec['cluster_score'] / max_cluster_score
                score = normalized_score * 0.4
                
                if paper_id_rec in paper_scores:
                    paper_scores[paper_id_rec]['total_score'] += score
                    paper_scores[paper_id_rec]['cluster_score'] = rec['cluster_score']
                    paper_scores[paper_id_rec]['methods'].append('cluster')
                else:
                    paper_scores[paper_id_rec] = {
                        'total_score': score,
                        'semantic_score': 0,
                        'cluster_score': rec['cluster_score'],
                        'methods': ['cluster']
                    }
            
            # ìƒìœ„ ì¶”ì²œ ì„ íƒ
            sorted_papers = sorted(paper_scores.items(), key=lambda x: x[1]['total_score'], reverse=True)
            
            hybrid_recs = []
            for i, (paper_id_rec, scores) in enumerate(sorted_papers[:n_recommendations]):
                hybrid_recs.append({
                    'paper_id': paper_id_rec,
                    'hybrid_score': float(scores['total_score']),
                    'semantic_score': float(scores['semantic_score']),
                    'cluster_score': float(scores['cluster_score']),
                    'methods': scores['methods'],
                    'rank': i + 1,
                    'method': 'hybrid'
                })
            
            logger.info(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ {len(hybrid_recs)}ê°œ ìƒì„±")
            return hybrid_recs
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return self.get_content_based_recommendations(paper_id, n_recommendations)

    def get_paper_details(self, paper_ids: List[str]) -> List[Dict]:
        """ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            placeholders = ','.join(['?' for _ in paper_ids])
            query = f"""
            SELECT paper_id, title, abstract, authors, categories, 
                   published_date, updated_date
            FROM papers 
            WHERE paper_id IN ({placeholders})
            """
            
            df = pd.read_sql_query(query, conn, params=paper_ids)
            conn.close()
            
            papers = []
            for _, row in df.iterrows():
                # JSON íŒŒì‹± ì²˜ë¦¬
                try:
                    authors = json.loads(row['authors']) if isinstance(row['authors'], str) else row['authors']
                    categories = json.loads(row['categories']) if isinstance(row['categories'], str) else row['categories']
                except:
                    authors = row['authors'].split(', ') if row['authors'] else []
                    categories = row['categories'].split(', ') if row['categories'] else []
                
                papers.append({
                    'paper_id': row['paper_id'],
                    'title': row['title'],
                    'abstract': row['abstract'][:300] + '...' if len(row['abstract']) > 300 else row['abstract'],
                    'authors': authors,
                    'categories': categories,
                    'published_date': row['published_date'],
                    'pdf_url': f"https://arxiv.org/pdf/{row['paper_id']}.pdf"
                })
            
            return papers
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def initialize_system(self, force_rebuild: bool = False):
        """ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("ğŸš€ í˜„ëŒ€ì  ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        embeddings_cache = os.path.join(self.model_cache_dir, 'specter2_embeddings.npy')
        index_cache = os.path.join(self.model_cache_dir, 'faiss_index.bin')
        clusters_cache = os.path.join(self.model_cache_dir, 'paper_clusters.npy')
        metadata_cache = os.path.join(self.model_cache_dir, 'metadata.json')
        
        if not force_rebuild and os.path.exists(embeddings_cache) and os.path.exists(metadata_cache):
            logger.info("ğŸ’¾ ìºì‹œì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            try:
                # ì„ë² ë”© ë¡œë“œ
                self.paper_embeddings = np.load(embeddings_cache)
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(metadata_cache, 'r') as f:
                    metadata = json.load(f)
                    self.paper_ids = metadata['paper_ids']
                
                # Faiss ì¸ë±ìŠ¤ ë¡œë“œ
                if os.path.exists(index_cache):
                    self.faiss_index = faiss.read_index(index_cache)
                else:
                    self.build_faiss_index(self.paper_embeddings)
                
                # í´ëŸ¬ìŠ¤í„° ë¡œë“œ
                if os.path.exists(clusters_cache):
                    self.paper_clusters = np.load(clusters_cache)
                    self._calculate_cluster_popularity()
                
                # papers_df ë¡œë“œ
                self.papers_df = self.load_papers_from_db()
                
                logger.info("âœ… ìºì‹œì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return
                
            except Exception as e:
                logger.warning(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ êµ¬ì¶•: {e}")
        
        # ìƒˆë¡œ êµ¬ì¶•
        papers_df = self.load_papers_from_db(limit=3000)  # ì„±ëŠ¥ê³¼ í’ˆì§ˆì˜ ê· í˜•
        
        if papers_df.empty:
            logger.error("âŒ ë…¼ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        self.papers_df = papers_df
        
        # SPECTER2 ì„ë² ë”© ìƒì„±
        embeddings = self.generate_paper_embeddings(papers_df)
        
        # Faiss ì¸ë±ìŠ¤ êµ¬ì¶•
        self.build_faiss_index(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        self.create_paper_clusters(embeddings)
        
        # ìºì‹œ ì €ì¥
        try:
            np.save(embeddings_cache, self.paper_embeddings)
            faiss.write_index(self.faiss_index, index_cache)
            np.save(clusters_cache, self.paper_clusters)
            
            metadata = {
                'paper_ids': self.paper_ids,
                'created_at': datetime.now().isoformat(),
                'paper_count': len(self.paper_ids),
                'model_type': 'SPECTER2'
            }
            
            with open(metadata_cache, 'w') as f:
                json.dump(metadata, f)
            
            logger.info("ğŸ’¾ ëª¨ë¸ ìºì‹œ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        logger.info("ğŸ‰ í˜„ëŒ€ì  ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

    def get_recommendations_for_paper(self, paper_id: str, recommendation_type: str = 'hybrid', n_recommendations: int = 10) -> Dict:
        """ë…¼ë¬¸ì— ëŒ€í•œ ì¶”ì²œ ìƒì„±"""
        if self.paper_embeddings is None:
            logger.error("âŒ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return {'error': 'ì¶”ì²œ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}
        
        if recommendation_type == 'content':
            recommendations = self.get_content_based_recommendations(paper_id, n_recommendations)
        elif recommendation_type == 'hybrid':
            recommendations = self.get_hybrid_recommendations(paper_id, n_recommendations)
        else:
            recommendations = self.get_content_based_recommendations(paper_id, n_recommendations)
        
        if not recommendations:
            return {'error': 'ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì¶”ì²œëœ ë…¼ë¬¸ë“¤ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        recommended_paper_ids = [rec['paper_id'] for rec in recommendations]
        paper_details = self.get_paper_details(recommended_paper_ids)
        
        # ì¶”ì²œ ì ìˆ˜ì™€ ë…¼ë¬¸ ì •ë³´ ê²°í•©
        result = []
        for rec in recommendations:
            paper_detail = next((p for p in paper_details if p['paper_id'] == rec['paper_id']), None)
            if paper_detail:
                paper_detail.update(rec)
                result.append(paper_detail)
        
        return {
            'recommendations': result,
            'total_count': len(result),
            'recommendation_type': recommendation_type,
            'source_paper_id': paper_id,
            'model_info': {
                'embedding_model': 'SPECTER2',
                'vector_search': 'Faiss',
                'clustering': 'K-means'
            }
        }

# ì „ì—­ ì¶”ì²œ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
recommendation_engine = None

def get_recommendation_engine():
    """ì¶”ì²œ ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global recommendation_engine
    if recommendation_engine is None:
        db_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'papers.db')
        model_cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'models')
        recommendation_engine = ModernRecommendationEngine(db_path, model_cache_dir)
    return recommendation_engine
