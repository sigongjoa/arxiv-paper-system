import os
import json
import sqlite3
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Tuple
from sentence_transformers import SentenceTransformer
import faiss
from datetime import datetime, timedelta
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
import logging

logger = logging.getLogger(__name__)

class ModernRecommendationEngine:
    def __init__(self, db_path: str = None, model_cache_dir: str = None):
        self.db_path = db_path or "papers.db"
        self.model_cache_dir = model_cache_dir or "models"
        
        os.makedirs(self.model_cache_dir, exist_ok=True)
        
        logger.info("SPECTER2 ëª¨ë¸ ë¡œë”© ì¤‘...")
        try:
            # SPECTER2: ë…¼ë¬¸ íŠ¹í™” ì„ë² ë”© ëª¨ë¸
            self.specter_model = SentenceTransformer('allenai/specter2')
            logger.info("âœ… SPECTER2 ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            logger.error(f"SPECTER2 ë¡œë“œ ì‹¤íŒ¨: {e}")
            # SciBERTë¡œ í´ë°±
            try:
                self.specter_model = SentenceTransformer('allenai/scibert_scivocab_uncased')
                logger.info("âœ… SciBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ (í´ë°±)")
            except Exception as e2:
                logger.error(f"SciBERTë„ ì‹¤íŒ¨: {e2}")
                # all-MiniLMìœ¼ë¡œ ìµœì¢… í´ë°±
                self.specter_model = SentenceTransformer('all-MiniLM-L6-v2')
                logger.info("âœ… MiniLM ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ìµœì¢… í´ë°±)")
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.paper_embeddings = None
        self.paper_ids = []
        self.papers_df = None
        self.faiss_index = None
        self.paper_clusters = None
        self.cluster_model = None
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ë°ì´í„°
        self.user_preferences = {}
        self.paper_popularity = {}
        
        logger.info("ğŸ¯ í˜„ëŒ€ì  ì¶”ì²œ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_papers_from_db(self, limit: int = None) -> pd.DataFrame:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            query = """
            SELECT paper_id, title, abstract, authors, categories, 
                   published_date, updated_date
            FROM papers 
            WHERE abstract IS NOT NULL AND title IS NOT NULL
            ORDER BY published_date DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            logger.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {len(df)}ê°œ ë…¼ë¬¸ ë¡œë“œ")
            return df
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def generate_paper_embeddings(self, papers_df: pd.DataFrame) -> np.ndarray:
        """SPECTER2ë¡œ ê³ í’ˆì§ˆ ë…¼ë¬¸ ì„ë² ë”© ìƒì„±"""
        logger.info("ğŸ§  SPECTER2 ì„ë² ë”© ìƒì„± ì‹œì‘...")
        
        # ë…¼ë¬¸ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ì´ˆë¡)
        paper_texts = []
        for _, row in papers_df.iterrows():
            # SPECTER2ì— ìµœì í™”ëœ í˜•ì‹
            text = f"Title: {row['title']}\nAbstract: {row['abstract']}"
            paper_texts.append(text)
        
        # SPECTER2 ì„ë² ë”© ìƒì„±
        embeddings = self.specter_model.encode(
            paper_texts,
            batch_size=8,  # ë©”ëª¨ë¦¬ ìµœì í™”
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”
        )
        
        self.paper_embeddings = embeddings
        self.paper_ids = papers_df['paper_id'].tolist()
        
        logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}")
        return embeddings

    def build_faiss_index(self, embeddings: np.ndarray):
        """Faissë¡œ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        logger.info("âš¡ Faiss ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # Inner Product ì¸ë±ìŠ¤ (ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼)
        index = faiss.IndexFlatIP(embeddings.shape[1])
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU ì¸ë±ìŠ¤ ì‚¬ìš©
        try:
            if faiss.get_num_gpus() > 0:
                index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index)
                logger.info("ğŸš€ GPU ê°€ì† ì¸ë±ìŠ¤ ì‚¬ìš©")
        except:
            logger.info("ğŸ’» CPU ì¸ë±ìŠ¤ ì‚¬ìš©")
        
        index.add(embeddings.astype('float32'))
        
        self.faiss_index = index
        logger.info(f"âœ… Faiss ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")

    def create_paper_clusters(self, embeddings: np.ndarray, n_clusters: int = 50):
        """ë…¼ë¬¸ì„ ì£¼ì œë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§"""
        effective_n_clusters = max(1, min(n_clusters, len(embeddings) // 2))
        logger.info(f"ğŸ¯ ë…¼ë¬¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (k={effective_n_clusters})...")
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        self.cluster_model = KMeans(n_clusters=effective_n_clusters, random_state=42, n_init=10)
        cluster_labels = self.cluster_model.fit_predict(embeddings)
        
        self.paper_clusters = cluster_labels
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ì¸ê¸°ë„ ê³„ì‚°
        self._calculate_cluster_popularity()
        
        logger.info(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {effective_n_clusters}ê°œ ì£¼ì œ í´ëŸ¬ìŠ¤í„°")

    def _calculate_cluster_popularity(self):
        """í´ëŸ¬ìŠ¤í„°ë³„ ì¸ê¸°ë„ ê³„ì‚° (ìµœì‹ ì„± + ë‹¤ì–‘ì„± ê¸°ë°˜)"""
        cluster_popularity = {}
        
        for cluster_id in np.unique(self.paper_clusters):
            cluster_papers = [i for i, c in enumerate(self.paper_clusters) if c == cluster_id]
            
            # ìµœì‹ ì„± ì ìˆ˜ ê³„ì‚°
            recency_scores = []
            for paper_idx in cluster_papers:
                try:
                    paper_date = datetime.fromisoformat(
                        self.papers_df.iloc[paper_idx]['published_date'].replace('Z', '+00:00')
                    )
                    days_old = (datetime.now() - paper_date.replace(tzinfo=None)).days
                    recency_score = max(0.1, 1.0 - (days_old / 365))  # 1ë…„ìœ¼ë¡œ ì •ê·œí™”
                    recency_scores.append(recency_score)
                except:
                    recency_scores.append(0.5)
            
            # í´ëŸ¬ìŠ¤í„° ì¸ê¸°ë„ = í‰ê·  ìµœì‹ ì„± + ë…¼ë¬¸ ìˆ˜ ê°€ì¤‘ì¹˜
            avg_recency = np.mean(recency_scores)
            size_weight = min(1.0, len(cluster_papers) / 10)  # 10ê°œ ì´ìƒì´ë©´ ìµœëŒ€ ê°€ì¤‘ì¹˜
            
            cluster_popularity[cluster_id] = avg_recency * 0.7 + size_weight * 0.3
        
        self.cluster_popularity = cluster_popularity

    def get_content_based_recommendations(self, paper_id: str, n_recommendations: int = 10) -> List[Dict]:
        """SPECTER2 ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¶”ì²œ"""
        try:
            if paper_id not in self.paper_ids:
                logger.warning(f"âŒ ë…¼ë¬¸ ID {paper_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            paper_idx = self.paper_ids.index(paper_id)
            query_embedding = self.paper_embeddings[paper_idx:paper_idx+1]
            
            # Faissë¡œ ìœ ì‚¬ ë…¼ë¬¸ ê²€ìƒ‰
            scores, indices = self.faiss_index.search(
                query_embedding.astype('float32'),
                n_recommendations + 1  # ìê¸° ìì‹  ì œì™¸
            )
            
            recommendations = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx != paper_idx:  # ìê¸° ìì‹  ì œì™¸
                    recommendations.append({
                        'paper_id': self.paper_ids[idx],
                        'similarity_score': float(score),
                        'rank': len(recommendations) + 1,
                        'method': 'semantic_similarity'
                    })
                
                if len(recommendations) >= n_recommendations:
                    break
            
            logger.info(f"ğŸ¯ ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¶”ì²œ {len(recommendations)}ê°œ ìƒì„±")
            return recommendations

        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return []

    def get_cluster_based_recommendations(self, paper_id: str, n_recommendations: int = 10) -> List[Dict]:
        """í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì¶”ì²œ (ì£¼ì œë³„ ì¸ê¸°ë„ ë°˜ì˜)"""
        try:
            if paper_id not in self.paper_ids or self.paper_clusters is None or self.cluster_popularity is None:
                return []

            paper_idx = self.paper_ids.index(paper_id)
            target_cluster_id = self.paper_clusters[paper_idx]
            
            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ë…¼ë¬¸ë“¤
            cluster_papers_indices = [i for i, c in enumerate(self.paper_clusters) if c == target_cluster_id]
            
            # í´ëŸ¬ìŠ¤í„° ì¸ê¸°ë„ ì ìˆ˜ ì‚¬ìš© (ë¯¸ë¦¬ ê³„ì‚°ëœ ê°’)
            cluster_score = self.cluster_popularity.get(target_cluster_id, 0.0)
            
            # í˜„ì¬ ë…¼ë¬¸ê³¼ ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì„ ê°€ì ¸ì˜´
            # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ê±°ë‚˜ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
            # ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œëŠ” í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œë„ ë” ì •êµí•œ ë­í‚¹ì´ í•„ìš”
            
            # ê°„ë‹¨í•˜ê²Œ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ë¬´ì‘ìœ„ë¡œ n_recommendations ë§Œí¼ ì„ íƒ
            # ìê¸° ìì‹  ì œì™¸
            candidates = [idx for idx in cluster_papers_indices if idx != paper_idx]
            if len(candidates) > n_recommendations:
                selected_indices = np.random.choice(candidates, n_recommendations, replace=False)
            else:
                selected_indices = candidates
            
            recommendations = []
            for idx in selected_indices:
                recommendations.append({
                    'paper_id': self.paper_ids[idx],
                    'cluster_score': float(cluster_score),
                    'method': 'cluster_based'
                })
            
            logger.info(f"ğŸ¯ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì¶”ì²œ {len(recommendations)}ê°œ ìƒì„±")
            return recommendations

        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return []

    def get_hybrid_recommendations(self, paper_id: str, n_recommendations: int = 10) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (ì½˜í…ì¸  ê¸°ë°˜ + í´ëŸ¬ìŠ¤í„° ê¸°ë°˜)"""
        try:
            content_recs = self.get_content_based_recommendations(paper_id, n_recommendations)
            cluster_recs = self.get_cluster_based_recommendations(paper_id, n_recommendations)
            
            combined_recs = {rec['paper_id']: rec for rec in content_recs}
            for rec in cluster_recs:
                if rec['paper_id'] in combined_recs:
                    # ì ìˆ˜ ë³‘í•© ë˜ëŠ” ê°€ì¤‘ì¹˜ ë¶€ì—¬
                    combined_recs[rec['paper_id']]['hybrid_score'] = (
                        combined_recs[rec['paper_id']].get('similarity_score', 0) * 0.7 +
                        rec.get('cluster_score', 0) * 0.3
                    )
                    combined_recs[rec['paper_id']]['methods'] = 'hybrid'
                else:
                    rec['hybrid_score'] = rec.get('cluster_score', 0)
                    rec['methods'] = 'cluster_based'
                    combined_recs[rec['paper_id']] = rec
            
            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬ ë° ìƒìœ„ Nê°œ ì„ íƒ
            final_recommendations = sorted(
                combined_recs.values(),
                key=lambda x: x.get('hybrid_score', 0), # hybrid_scoreê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ê°„ì£¼
                reverse=True
            )[:n_recommendations]
            
            # ì œëª© ì •ë³´ ì¶”ê°€ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ í•œ ë²ˆì— ì¡°íšŒ)
            paper_ids_to_fetch = [rec['paper_id'] for rec in final_recommendations]
            paper_details = self.get_paper_details(paper_ids_to_fetch)
            paper_details_map = {p['paper_id']: p for p in paper_details}

            for rec in final_recommendations:
                detail = paper_details_map.get(rec['paper_id'])
                if detail:
                    rec['title'] = detail.get('title')
                    rec['abstract'] = detail.get('abstract')
                    rec['authors'] = detail.get('authors')
                    rec['categories'] = detail.get('categories')
                    rec['published_date'] = detail.get('published_date')
            
            logger.info(f"ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ {len(final_recommendations)}ê°œ ìƒì„±")
            return final_recommendations

        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return []

    def get_paper_details(self, paper_ids: List[str]) -> List[Dict]:
        """ì£¼ì–´ì§„ ë…¼ë¬¸ ID ëª©ë¡ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ê²€ìƒ‰"""
        if not paper_ids:
            return []

        try:
            conn = sqlite3.connect(self.db_path)
            placeholders = ', '.join(['?' for _ in paper_ids])
            query = f"SELECT paper_id, title, abstract, authors, categories, published_date, updated_date FROM papers WHERE paper_id IN ({placeholders})"
            
            cursor = conn.execute(query, paper_ids)
            columns = [column[0] for column in cursor.description]
            results = [dict(zip(columns, row)) for row in cursor.fetchall()]
            conn.close()
            
            # JSON í•„ë“œ íŒŒì‹±
            for row in results:
                if 'authors' in row and row['authors']:
                    row['authors'] = json.loads(row['authors'])
                if 'categories' in row and row['categories']:
                    row['categories'] = json.loads(row['categories'])
            
            logger.info(f"ë…¼ë¬¸ ìƒì„¸ ì •ë³´ {len(results)}ê°œ ì¡°íšŒ")
            return results
            
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
            return []

    def initialize_system(self, force_rebuild: bool = False):
        """ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”: ë…¼ë¬¸ ë¡œë“œ, ì„ë² ë”© ìƒì„±, Faiss ì¸ë±ìŠ¤ êµ¬ì¶•, í´ëŸ¬ìŠ¤í„°ë§"""
        logger.info("ğŸš€ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(self.db_path):
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.db_path}")
            raise FileNotFoundError(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")

        # ì„ë² ë”© ë° ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        embeddings_path = os.path.join(self.model_cache_dir, "paper_embeddings.npy")
        faiss_index_path = os.path.join(self.model_cache_dir, "faiss_index.bin")
        paper_ids_path = os.path.join(self.model_cache_dir, "paper_ids.json")
        clusters_path = os.path.join(self.model_cache_dir, "paper_clusters.npy")
        cluster_model_path = os.path.join(self.model_cache_dir, "cluster_model.pkl")
        
        # ìºì‹œëœ ë°ì´í„° ë¡œë“œ ì‹œë„
        if not force_rebuild and \
           os.path.exists(embeddings_path) and \
           os.path.exists(faiss_index_path) and \
           os.path.exists(paper_ids_path) and \
           os.path.exists(clusters_path) and \
           os.path.exists(cluster_model_path):
            try:
                self.paper_embeddings = np.load(embeddings_path)
                self.faiss_index = faiss.read_index(faiss_index_path)
                with open(paper_ids_path, 'r') as f:
                    self.paper_ids = json.load(f)
                self.paper_clusters = np.load(clusters_path)
                with open(cluster_model_path, 'rb') as f:
                    import pickle
                    self.cluster_model = pickle.load(f) # scikit-learn ëª¨ë¸ ë¡œë“œ
                
                self.papers_df = self.load_papers_from_db() # DataFrameì€ í•­ìƒ DBì—ì„œ ë¡œë“œ
                self._calculate_cluster_popularity() # í´ëŸ¬ìŠ¤í„° ì¸ê¸°ë„ ë‹¤ì‹œ ê³„ì‚° (ìµœì‹  ë°ì´í„° ë°˜ì˜)
                
                logger.info("âœ… ìºì‹œëœ ì„ë² ë”©, ì¸ë±ìŠ¤ ë° í´ëŸ¬ìŠ¤í„° ë¡œë“œ ì„±ê³µ")
                return
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ({e}), ì‹œìŠ¤í…œ ì¬êµ¬ì¶• ì‹œì‘...")
        
        # ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
        self.papers_df = self.load_papers_from_db()
        if self.papers_df.empty:
            logger.warning("ë°ì´í„°ë² ì´ìŠ¤ì— ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì²œ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        embeddings = self.generate_paper_embeddings(self.papers_df)
        self.build_faiss_index(embeddings)
        self.create_paper_clusters(embeddings) # í´ëŸ¬ìŠ¤í„°ë§ì„ ë¨¼ì € ìˆ˜í–‰
        
        # ìºì‹œ ì €ì¥
        try:
            np.save(embeddings_path, embeddings)
            faiss.write_index(self.faiss_index, faiss_index_path)
            with open(paper_ids_path, 'w') as f:
                json.dump(self.paper_ids, f)
            np.save(clusters_path, self.paper_clusters)
            with open(cluster_model_path, 'wb') as f:
                import pickle
                pickle.dump(self.cluster_model, f) # scikit-learn ëª¨ë¸ ì €ì¥
            logger.info("âœ… ì„ë² ë”©, ì¸ë±ìŠ¤ ë° í´ëŸ¬ìŠ¤í„° ìºì‹œ ì €ì¥ ì„±ê³µ")
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

        logger.info("âœ… ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def get_recommendations_for_paper(self, paper_id: str, recommendation_type: str = 'hybrid', n_recommendations: int = 10) -> Dict:
        """ì£¼ì–´ì§„ ë…¼ë¬¸ IDì— ëŒ€í•œ ì¶”ì²œì„ ìƒì„±"""
        if paper_id not in self.paper_ids:
            return {"error": f"ë…¼ë¬¸ ID '{paper_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "recommendations": []}

        if recommendation_type == 'content_based':
            recs = self.get_content_based_recommendations(paper_id, n_recommendations)
        elif recommendation_type == 'cluster_based':
            recs = self.get_cluster_based_recommendations(paper_id, n_recommendations)
        elif recommendation_type == 'hybrid':
            recs = self.get_hybrid_recommendations(paper_id, n_recommendations)
        else:
            return {"error": "ìœ íš¨í•˜ì§€ ì•Šì€ ì¶”ì²œ ìœ í˜•ì…ë‹ˆë‹¤.", "recommendations": []}
        
        if not recs:
            return {"error": "ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "recommendations": []}
            
        return {"recommendations": recs}

def get_recommendation_engine():
    """ì‹±ê¸€í†¤ ì¶”ì²œ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _recommendation_engine_instance
    if _recommendation_engine_instance is None:
        db_path = os.path.join(os.path.dirname(__file__), 'arxiv_papers.db')
        model_cache_dir = os.path.join(os.path.dirname(__file__), 'models')
        _recommendation_engine_instance = ModernRecommendationEngine(db_path=db_path, model_cache_dir=model_cache_dir)
        _recommendation_engine_instance.initialize_system()
    return _recommendation_engine_instance

_recommendation_engine_instance = None 